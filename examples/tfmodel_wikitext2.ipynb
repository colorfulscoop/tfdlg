{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfchat.utils import set_memory_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set memory growth to PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "set_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.1.91)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (4.50.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (20.4)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (2020.10.15)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.9.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (2.24.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.4.0) (50.3.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0) (0.17.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==3.4.0) (2.6)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install transformers by HuggingFace to use GPT2 tokenizer\n",
    "! pip install transformers==3.4.0\n",
    "# Enable widgetsnbextention to avoid the following error when running GPT2.from_pretrained method\n",
    "#     ImportError: IProgress not found. Please update jupyter and ipywidgets.\n",
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfchat.configs import GPT2SmallConfig\n",
    "\n",
    "config = GPT2SmallConfig()\n",
    "\n",
    "# Set the larger number of vocab size than 33,278, which is the vocab size of Wikitext-2\n",
    "config.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2SmallConfig(num_layers=12, d_model=768, num_heads=12, d_ff=3072, vocab_size=50257, context_size=1024, attention_dropout_rate=0.1, residual_dropout_rate=0.1, embedding_dropout_rate=0.1, epsilon=1e-06)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_file(tokenizer, filepath):\n",
    "    ids = []\n",
    "    with open(filepath) as f:\n",
    "        for line in f.readlines():\n",
    "            text = line.strip(\"\\n\")\n",
    "            ids.extend(tokenizer.encode(text))\n",
    "\n",
    "    return np.array(ids, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = encode_file(tokenizer, \"wikitext-2/wiki.train.tokens\")\n",
    "valid_ids = encode_file(tokenizer, \"wikitext-2/wiki.valid.tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2398713,)\n",
      "Valid: (253600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", train_ids.shape)\n",
    "print(\"Valid:\", valid_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2398713,)\n",
      "(253600,)\n"
     ]
    }
   ],
   "source": [
    "print(train_ids.shape)\n",
    "print(valid_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfchat.data import BlockDataset\n",
    "\n",
    "\n",
    "dataset = BlockDataset(block_size=config.context_size, batch_size=2)\n",
    "\n",
    "train_dataset = dataset.build(train_ids, shuffle=True)\n",
    "test_dataset = dataset.build(valid_ids, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Train size:\", len(train_dataset))\n",
    "#print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfchat.metrics import perplexity\n",
    "from tfchat.losses import PaddingLoss\n",
    "from tfchat.optimizers import TransformerScheduler\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model.compile(loss=PaddingLoss(),\n",
    "                  optimizer=keras.optimizers.Adam(TransformerScheduler(d_model=config.d_model, warmup_steps=800),\n",
    "                                                  beta_1=0.9,\n",
    "                                                  beta_2=0.999,\n",
    "                                                  epsilon=1e-8,\n",
    "                                                  clipnorm=1.0,\n",
    "                                                 ),\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                 )\n",
    "    model.build(input_shape=(None, config.context_size))\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=20,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True),\n",
    "            keras.callbacks.ModelCheckpoint(\"keras_model\", save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "    return perplexity(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PostLN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tfchat.models import PostLNDecoder \n",
    "\n",
    "#model = PostLNDecoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PreLN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfchat.models import PreLNDecoder\n",
    "model = PreLNDecoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pre_ln_decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder (Decoder)            multiple                  162299473 \n",
      "=================================================================\n",
      "Total params: 162,299,473\n",
      "Trainable params: 162,299,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1171/1171 [==============================] - 600s 512ms/step - loss: 6.1857 - sparse_categorical_accuracy: 0.1916 - val_loss: 5.1655 - val_sparse_categorical_accuracy: 0.2671\n",
      "Epoch 2/20\n",
      "1171/1171 [==============================] - 603s 515ms/step - loss: 5.0052 - sparse_categorical_accuracy: 0.2469 - val_loss: 4.7342 - val_sparse_categorical_accuracy: 0.2882\n",
      "Epoch 3/20\n",
      "1171/1171 [==============================] - 610s 521ms/step - loss: 4.2042 - sparse_categorical_accuracy: 0.2796 - val_loss: 4.6299 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 4/20\n",
      "1171/1171 [==============================] - 605s 516ms/step - loss: 3.6877 - sparse_categorical_accuracy: 0.3126 - val_loss: 4.6492 - val_sparse_categorical_accuracy: 0.3032\n",
      "{'loss': 4.629918, 'perplexity': 102.50567, 'num_batches': 123, 'num_tokens': 251904}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102.50567"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am \"'s great great feeling the time and <unk>. The use of £ 3 million injuries\n"
     ]
    }
   ],
   "source": [
    "from tfchat.generations import TopKTopPGenerator\n",
    "\n",
    "gen = TopKTopPGenerator(model=model, max_len=20)\n",
    "inputs = np.array([tokenizer.encode(\"I am\")], dtype=np.int32)\n",
    "\n",
    "outputs = gen.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with minGPT-TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kamalkraj/minGPT-TF\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "\n",
    "mconf = GPTConfig(config.vocab_size, config.context_size,\n",
    "                  n_layer=config.num_layers, n_head=config.num_heads, n_embd=config.d_model)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  38597376  \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "encoder_layer (EncoderLayer) multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_1 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_2 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_3 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_4 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_5 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_6 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_7 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_8 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_9 (EncoderLaye multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_10 (EncoderLay multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "encoder_layer_11 (EncoderLay multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "layer_normalization_48 (Laye multiple                  1536      \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             multiple                  38597376  \n",
      "=================================================================\n",
      "Total params: 163,037,184\n",
      "Trainable params: 163,037,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1171/1171 [==============================] - 641s 548ms/step - loss: 6.1741 - sparse_categorical_accuracy: 0.1846 - val_loss: 5.1427 - val_sparse_categorical_accuracy: 0.2623\n",
      "Epoch 2/20\n",
      "1171/1171 [==============================] - 648s 553ms/step - loss: 5.4263 - sparse_categorical_accuracy: 0.2138 - val_loss: 5.0218 - val_sparse_categorical_accuracy: 0.2625\n",
      "Epoch 3/20\n",
      "1171/1171 [==============================] - 644s 550ms/step - loss: 5.3479 - sparse_categorical_accuracy: 0.2109 - val_loss: 5.0704 - val_sparse_categorical_accuracy: 0.2557\n",
      "{'loss': 5.021758, 'perplexity': 151.67773, 'num_batches': 123, 'num_tokens': 251904}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151.67773"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
