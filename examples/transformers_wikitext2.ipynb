{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install HuggingFace's transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.4.0 in /opt/conda/lib/python3.7/site-packages (3.4.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (2020.10.15)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (1.18.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (0.1.91)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (4.46.0)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (0.9.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (20.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (0.17.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (2.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf->transformers==3.4.0) (46.4.0.post20200518)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run_language_modeling.py', <http.client.HTTPMessage at 0x7fca06c70090>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/huggingface/transformers/v3.4.0/examples/language-modeling/run_language_modeling.py\"\n",
    "urllib.request.urlretrieve(url, \"run_language_modeling.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"wikitext-2-raw/wiki.train.raw\"\n",
    "test_file = \"wikitext-2-raw/wiki.valid.raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n",
      "10/22/2020 16:25:49 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "10/22/2020 16:25:49 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct22_16-25-49_cbe7f4ec77fb', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output', disable_tqdm='False', remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n",
      "10/22/2020 16:25:49 - WARNING - __main__ -   You are instantiating a new config instance from scratch.\n",
      "10/22/2020 16:25:52 - INFO - __main__ -   Training new model from scratch\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1374: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n",
      "10/22/2020 16:25:54 - INFO - filelock -   Lock 140656024766800 acquired on wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.train.raw.lock\n",
      "10/22/2020 16:25:54 - INFO - filelock -   Lock 140656024766800 released on wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.train.raw.lock\n",
      "10/22/2020 16:25:54 - INFO - filelock -   Lock 140656024546704 acquired on wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.valid.raw.lock\n",
      "10/22/2020 16:25:55 - INFO - filelock -   Lock 140656024546704 released on wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.valid.raw.lock\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:263: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
      "  FutureWarning,\n",
      "{'loss': 7.13867822265625, 'learning_rate': 4.7883149872988994e-05, 'epoch': 0.42337002540220153}\n",
      "{'eval_loss': 6.609846115112305, 'epoch': 0.42337002540220153}\n",
      "{'loss': 6.45258251953125, 'learning_rate': 4.5766299745977986e-05, 'epoch': 0.8467400508044031}\n",
      "{'eval_loss': 6.326825141906738, 'epoch': 0.8467400508044031}\n",
      "{'loss': 6.1865673828125, 'learning_rate': 4.364944961896698e-05, 'epoch': 1.2701100762066047}\n",
      "{'eval_loss': 6.161691188812256, 'epoch': 1.2701100762066047}\n",
      "{'loss': 6.011833984375, 'learning_rate': 4.153259949195597e-05, 'epoch': 1.6934801016088061}\n",
      "{'eval_loss': 6.050586223602295, 'epoch': 1.6934801016088061}\n",
      "{'loss': 5.888916015625, 'learning_rate': 3.941574936494496e-05, 'epoch': 2.116850127011008}\n",
      "{'eval_loss': 5.977848529815674, 'epoch': 2.116850127011008}\n",
      "{'loss': 5.73905859375, 'learning_rate': 3.7298899237933954e-05, 'epoch': 2.5402201524132093}\n",
      "{'eval_loss': 5.910695552825928, 'epoch': 2.5402201524132093}\n",
      "{'loss': 5.68698046875, 'learning_rate': 3.5182049110922946e-05, 'epoch': 2.963590177815411}\n",
      "{'eval_loss': 5.852723121643066, 'epoch': 2.963590177815411}\n",
      "{'loss': 5.54437890625, 'learning_rate': 3.306519898391194e-05, 'epoch': 3.3869602032176123}\n",
      "{'eval_loss': 5.817033767700195, 'epoch': 3.3869602032176123}\n",
      "{'loss': 5.49662890625, 'learning_rate': 3.094834885690093e-05, 'epoch': 3.8103302286198137}\n",
      "{'eval_loss': 5.765716552734375, 'epoch': 3.8103302286198137}\n",
      "{'loss': 5.42927734375, 'learning_rate': 2.883149872988993e-05, 'epoch': 4.233700254022016}\n",
      "{'eval_loss': 5.73885440826416, 'epoch': 4.233700254022016}\n",
      "{'loss': 5.34473828125, 'learning_rate': 2.6714648602878917e-05, 'epoch': 4.657070279424217}\n",
      "{'eval_loss': 5.7044997215271, 'epoch': 4.657070279424217}\n",
      "{'loss': 5.2932734375, 'learning_rate': 2.459779847586791e-05, 'epoch': 5.080440304826419}\n",
      "{'eval_loss': 5.682712078094482, 'epoch': 5.080440304826419}\n",
      "{'loss': 5.193765625, 'learning_rate': 2.2480948348856904e-05, 'epoch': 5.50381033022862}\n",
      "{'eval_loss': 5.661936283111572, 'epoch': 5.50381033022862}\n",
      "{'loss': 5.188625, 'learning_rate': 2.0364098221845896e-05, 'epoch': 5.927180355630822}\n",
      "{'eval_loss': 5.6358160972595215, 'epoch': 5.927180355630822}\n",
      "{'loss': 5.088875, 'learning_rate': 1.8247248094834888e-05, 'epoch': 6.350550381033023}\n",
      "{'eval_loss': 5.622176170349121, 'epoch': 6.350550381033023}\n",
      "{'loss': 5.08315625, 'learning_rate': 1.613039796782388e-05, 'epoch': 6.7739204064352245}\n",
      "{'eval_loss': 5.602283954620361, 'epoch': 6.7739204064352245}\n",
      "{'loss': 5.0347265625, 'learning_rate': 1.4013547840812872e-05, 'epoch': 7.197290431837426}\n",
      "{'eval_loss': 5.593043804168701, 'epoch': 7.197290431837426}\n",
      "{'loss': 4.9859296875, 'learning_rate': 1.1896697713801864e-05, 'epoch': 7.6206604572396275}\n",
      "{'eval_loss': 5.582730770111084, 'epoch': 7.6206604572396275}\n",
      "{'loss': 4.9586484375, 'learning_rate': 9.779847586790856e-06, 'epoch': 8.04403048264183}\n",
      "{'eval_loss': 5.571876525878906, 'epoch': 8.04403048264183}\n",
      "{'loss': 4.90128125, 'learning_rate': 7.66299745977985e-06, 'epoch': 8.467400508044031}\n",
      "{'eval_loss': 5.56473445892334, 'epoch': 8.467400508044031}\n",
      "{'loss': 4.9215703125, 'learning_rate': 5.54614733276884e-06, 'epoch': 8.890770533446233}\n",
      "{'eval_loss': 5.555153846740723, 'epoch': 8.890770533446233}\n",
      "{'loss': 4.859375, 'learning_rate': 3.429297205757832e-06, 'epoch': 9.314140558848434}\n",
      "{'eval_loss': 5.552412033081055, 'epoch': 9.314140558848434}\n",
      "{'loss': 4.8678125, 'learning_rate': 1.312447078746825e-06, 'epoch': 9.737510584250636}\n",
      "{'eval_loss': 5.5479607582092285, 'epoch': 9.737510584250636}\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
      "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
      "10/22/2020 17:29:21 - INFO - __main__ -   *** Evaluate ***\n",
      "{'eval_loss': 5.5479607582092285, 'epoch': 10.0}\n",
      "10/22/2020 17:29:32 - INFO - __main__ -   ***** Eval results *****\n",
      "10/22/2020 17:29:32 - INFO - __main__ -     perplexity = 256.71352083032895\n"
     ]
    }
   ],
   "source": [
    "# Train model from scratch\n",
    "!python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=gpt2 \\\n",
    "    --tokenizer_name=gpt2 \\\n",
    "    --do_train \\\n",
    "    --train_data_file={train_file} \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file={test_file} \\\n",
    "    --per_device_train_batch_size=2 \\\n",
    "    --num_train_epochs=10 \\\n",
    "    --evaluate_during_training \\\n",
    "    --load_best_model_at_end \\\n",
    "    --disable_tqdm=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
