{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hourly-colony",
   "metadata": {
    "papermill": {
     "duration": 0.014993,
     "end_time": "2021-03-07T02:50:50.589625",
     "exception": false,
     "start_time": "2021-03-07T02:50:50.574632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "related-sellers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:50.621115Z",
     "iopub.status.busy": "2021-03-07T02:50:50.620738Z",
     "iopub.status.idle": "2021-03-07T02:50:50.622697Z",
     "shell.execute_reply": "2021-03-07T02:50:50.622325Z"
    },
    "papermill": {
     "duration": 0.020367,
     "end_time": "2021-03-07T02:50:50.622776",
     "exception": false,
     "start_time": "2021-03-07T02:50:50.602409",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# These parameters can be injected from Papermill\n",
    "model_type = \"pre_ln\"\n",
    "train_file = \"wikitext-103-raw/wiki.train.raw\"\n",
    "valid_file = \"wikitext-103-raw/wiki.valid.raw\"\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "max_learning_rate = 1e-4\n",
    "warmup_steps = 0\n",
    "clipnorm = 1.0\n",
    "fp16 = False\n",
    "save_model_dir = f\"output/tfdlg_train-{model_type}-model\"\n",
    "tensorboard_dir = f\"output/tensorboard/{save_model_dir}-tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electrical-chain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:50.648600Z",
     "iopub.status.busy": "2021-03-07T02:50:50.648217Z",
     "iopub.status.idle": "2021-03-07T02:50:50.649968Z",
     "shell.execute_reply": "2021-03-07T02:50:50.649601Z"
    },
    "papermill": {
     "duration": 0.015437,
     "end_time": "2021-03-07T02:50:50.650040",
     "exception": false,
     "start_time": "2021-03-07T02:50:50.634603",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_type = \"pre_ln\"\n",
    "batch_size = 4\n",
    "fp16 = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proud-reputation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:50.676117Z",
     "iopub.status.busy": "2021-03-07T02:50:50.675746Z",
     "iopub.status.idle": "2021-03-07T02:50:50.677578Z",
     "shell.execute_reply": "2021-03-07T02:50:50.677198Z"
    },
    "papermill": {
     "duration": 0.01563,
     "end_time": "2021-03-07T02:50:50.677678",
     "exception": false,
     "start_time": "2021-03-07T02:50:50.662048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assert parameters\n",
    "assert model_type in [\"pre_ln\", \"post_ln\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-chile",
   "metadata": {
    "papermill": {
     "duration": 0.012462,
     "end_time": "2021-03-07T02:50:50.702603",
     "exception": false,
     "start_time": "2021-03-07T02:50:50.690141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acquired-agency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:50.730098Z",
     "iopub.status.busy": "2021-03-07T02:50:50.729727Z",
     "iopub.status.idle": "2021-03-07T02:50:51.604414Z",
     "shell.execute_reply": "2021-03-07T02:50:51.604020Z"
    },
    "papermill": {
     "duration": 0.88928,
     "end_time": "2021-03-07T02:50:51.604495",
     "exception": false,
     "start_time": "2021-03-07T02:50:50.715215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfdlg.utils import set_memory_growth\n",
    "from tfdlg.utils import set_mixed_precision_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "changing-fence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:51.703488Z",
     "iopub.status.busy": "2021-03-07T02:50:51.702932Z",
     "iopub.status.idle": "2021-03-07T02:50:51.705309Z",
     "shell.execute_reply": "2021-03-07T02:50:51.705751Z"
    },
    "papermill": {
     "duration": 0.089227,
     "end_time": "2021-03-07T02:50:51.705881",
     "exception": false,
     "start_time": "2021-03-07T02:50:51.616654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set memory growth to PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "set_memory_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compressed-destiny",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:51.765944Z",
     "iopub.status.busy": "2021-03-07T02:50:51.765521Z",
     "iopub.status.idle": "2021-03-07T02:50:52.102469Z",
     "shell.execute_reply": "2021-03-07T02:50:52.102093Z"
    },
    "papermill": {
     "duration": 0.381391,
     "end_time": "2021-03-07T02:50:52.102552",
     "exception": false,
     "start_time": "2021-03-07T02:50:51.721161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 2080 Ti, compute capability 7.5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "if fp16:\n",
    "    set_mixed_precision_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-football",
   "metadata": {
    "papermill": {
     "duration": 0.012891,
     "end_time": "2021-03-07T02:50:52.128862",
     "exception": false,
     "start_time": "2021-03-07T02:50:52.115971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "partial-question",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:52.158423Z",
     "iopub.status.busy": "2021-03-07T02:50:52.157946Z",
     "iopub.status.idle": "2021-03-07T02:50:53.568212Z",
     "shell.execute_reply": "2021-03-07T02:50:53.566381Z"
    },
    "papermill": {
     "duration": 1.426852,
     "end_time": "2021-03-07T02:50:53.568648",
     "exception": false,
     "start_time": "2021-03-07T02:50:52.141796",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (4.59.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.1.91)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (3.14.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (1.19.5)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (2.25.1)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (0.9.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0) (2020.11.13)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==3.4.0) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0) (2020.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install transformers by HuggingFace to use GPT2 tokenizer\n",
    "! pip install transformers==3.4.0\n",
    "# Enable widgetsnbextention to avoid the following error when running GPT2.from_pretrained method\n",
    "#     ImportError: IProgress not found. Please update jupyter and ipywidgets.\n",
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "potential-forum",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:53.600344Z",
     "iopub.status.busy": "2021-03-07T02:50:53.599930Z",
     "iopub.status.idle": "2021-03-07T02:50:55.278186Z",
     "shell.execute_reply": "2021-03-07T02:50:55.277801Z"
    },
    "papermill": {
     "duration": 1.694513,
     "end_time": "2021-03-07T02:50:55.278267",
     "exception": false,
     "start_time": "2021-03-07T02:50:53.583754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-pension",
   "metadata": {
    "papermill": {
     "duration": 0.013768,
     "end_time": "2021-03-07T02:50:55.306240",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.292472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weekly-metropolitan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:55.336858Z",
     "iopub.status.busy": "2021-03-07T02:50:55.336490Z",
     "iopub.status.idle": "2021-03-07T02:50:55.368778Z",
     "shell.execute_reply": "2021-03-07T02:50:55.368392Z"
    },
    "papermill": {
     "duration": 0.048777,
     "end_time": "2021-03-07T02:50:55.368859",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.320082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfdlg.configs import GPT2SmallConfig\n",
    "\n",
    "config = GPT2SmallConfig()\n",
    "\n",
    "# Set the larger number of vocab size than 33,278, which is the vocab size of Wikitext-2\n",
    "config.vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continent-galaxy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:55.402763Z",
     "iopub.status.busy": "2021-03-07T02:50:55.402365Z",
     "iopub.status.idle": "2021-03-07T02:50:55.404206Z",
     "shell.execute_reply": "2021-03-07T02:50:55.404502Z"
    },
    "papermill": {
     "duration": 0.021755,
     "end_time": "2021-03-07T02:50:55.404586",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.382831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2SmallConfig(num_layers=12, d_model=768, num_heads=12, d_ff=3072, vocab_size=50257, context_size=1024, attention_dropout_rate=0.1, residual_dropout_rate=0.1, embedding_dropout_rate=0.1, activation='gelu', kernel_initializer='he_normal', epsilon=1e-06)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-lebanon",
   "metadata": {
    "papermill": {
     "duration": 0.014111,
     "end_time": "2021-03-07T02:50:55.432905",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.418794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entitled-toilet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:55.463605Z",
     "iopub.status.busy": "2021-03-07T02:50:55.463233Z",
     "iopub.status.idle": "2021-03-07T02:50:55.464714Z",
     "shell.execute_reply": "2021-03-07T02:50:55.465004Z"
    },
    "papermill": {
     "duration": 0.018352,
     "end_time": "2021-03-07T02:50:55.465085",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.446733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_file(_filepath):\n",
    "    return (t.strip(\"\\n\") for t in open(_filepath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "egyptian-retreat",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:55.499607Z",
     "iopub.status.busy": "2021-03-07T02:50:55.499231Z",
     "iopub.status.idle": "2021-03-07T02:50:55.564142Z",
     "shell.execute_reply": "2021-03-07T02:50:55.563754Z"
    },
    "papermill": {
     "duration": 0.08512,
     "end_time": "2021-03-07T02:50:55.564224",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.479104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfdlg.data import BlockDataset\n",
    "\n",
    "\n",
    "train_dataset = BlockDataset.from_generator(\n",
    "    generator=lambda: read_file(train_file),\n",
    "    encode_fn=tokenizer.encode,\n",
    "    block_size=config.context_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_dataset = BlockDataset.from_generator(\n",
    "    generator=lambda: read_file(valid_file),\n",
    "    encode_fn=tokenizer.encode,\n",
    "    block_size=config.context_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smooth-wallet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T02:50:55.595490Z",
     "iopub.status.busy": "2021-03-07T02:50:55.595118Z",
     "iopub.status.idle": "2021-03-07T04:05:05.671634Z",
     "shell.execute_reply": "2021-03-07T04:05:05.671942Z"
    },
    "papermill": {
     "duration": 4450.093498,
     "end_time": "2021-03-07T04:05:05.672031",
     "exception": false,
     "start_time": "2021-03-07T02:50:55.578533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train steps: 28504\n",
      "Valid steps: 59\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = sum(1 for _ in train_dataset)\n",
    "num_valid_steps = sum(1 for _ in valid_dataset)\n",
    "print(\"Train steps:\", num_train_steps)\n",
    "print(\"Valid steps:\", num_valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-brunei",
   "metadata": {
    "papermill": {
     "duration": 0.014587,
     "end_time": "2021-03-07T04:05:05.701137",
     "exception": false,
     "start_time": "2021-03-07T04:05:05.686550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transformers model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "above-helicopter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T04:05:05.733265Z",
     "iopub.status.busy": "2021-03-07T04:05:05.732907Z",
     "iopub.status.idle": "2021-03-07T04:05:05.734865Z",
     "shell.execute_reply": "2021-03-07T04:05:05.734505Z"
    },
    "papermill": {
     "duration": 0.019019,
     "end_time": "2021-03-07T04:05:05.734937",
     "exception": false,
     "start_time": "2021-03-07T04:05:05.715918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Config\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "special-stage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T04:05:05.769182Z",
     "iopub.status.busy": "2021-03-07T04:05:05.768815Z",
     "iopub.status.idle": "2021-03-07T04:05:05.770727Z",
     "shell.execute_reply": "2021-03-07T04:05:05.770430Z"
    },
    "papermill": {
     "duration": 0.020936,
     "end_time": "2021-03-07T04:05:05.770799",
     "exception": false,
     "start_time": "2021-03-07T04:05:05.749863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformersGPT2(keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        tf_config = GPT2Config(\n",
    "            n_layers=config.num_layers,\n",
    "            n_embd=config.d_model,\n",
    "            n_head=config.num_heads,\n",
    "            n_inner=config.d_ff,\n",
    "            vocab_size=config.vocab_size,\n",
    "            n_ctx=config.context_size,\n",
    "            n_positions=config.context_size,\n",
    "            attn_pdrop=config.attention_dropout_rate,\n",
    "            resid_pdrop=config.residual_dropout_rate,\n",
    "            embd_pdrop=config.embedding_dropout_rate,\n",
    "            layer_norm_epsilon=config.epsilon,\n",
    "            activation_function=\"gelu_new\",  # Default value of transformers implementation\n",
    "            \n",
    "        )\n",
    "        self._decoder = TFGPT2LMHeadModel(tf_config)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        inputs = tf.cast(inputs, tf.int32)\n",
    "        x = self._decoder(inputs, training=training)\n",
    "        return x[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-possession",
   "metadata": {
    "papermill": {
     "duration": 0.014893,
     "end_time": "2021-03-07T04:05:05.800368",
     "exception": false,
     "start_time": "2021-03-07T04:05:05.785475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "equal-astrology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T04:05:05.836685Z",
     "iopub.status.busy": "2021-03-07T04:05:05.836249Z",
     "iopub.status.idle": "2021-03-07T04:05:05.838108Z",
     "shell.execute_reply": "2021-03-07T04:05:05.837740Z"
    },
    "papermill": {
     "duration": 0.022527,
     "end_time": "2021-03-07T04:05:05.838187",
     "exception": false,
     "start_time": "2021-03-07T04:05:05.815660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfdlg.losses import PaddingLoss\n",
    "from tfdlg.schedules import WarmupLinearDecay\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    _model,\n",
    "    _train_dataset,\n",
    "    _valid_dataset,\n",
    "    _epochs,\n",
    "    _warmup_steps,\n",
    "    _num_train_steps,\n",
    "    _max_learning_rate,\n",
    "    _clipnorm,\n",
    "    _tensorboard_dir\n",
    "):\n",
    "    schedule = WarmupLinearDecay(\n",
    "        max_learning_rate=_max_learning_rate,\n",
    "        warmup_steps=_warmup_steps,\n",
    "        training_steps=_num_train_steps*_epochs\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=_clipnorm)\n",
    "    _model.compile(loss=PaddingLoss(), optimizer=optimizer)\n",
    "\n",
    "    history = _model.fit(\n",
    "        _train_dataset,\n",
    "        validation_data=_valid_dataset,\n",
    "        epochs=_epochs,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True),\n",
    "            keras.callbacks.TensorBoard(\n",
    "                log_dir=tensorboard_dir,\n",
    "                update_freq=100,\n",
    "                profile_batch=0,\n",
    "            )\n",
    "        ],\n",
    "        verbose=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "regulated-treat",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T04:05:05.883988Z",
     "iopub.status.busy": "2021-03-07T04:05:05.878535Z",
     "iopub.status.idle": "2021-03-07T04:05:06.019977Z",
     "shell.execute_reply": "2021-03-07T04:05:06.019595Z"
    },
    "papermill": {
     "duration": 0.166671,
     "end_time": "2021-03-07T04:05:06.020056",
     "exception": false,
     "start_time": "2021-03-07T04:05:05.853385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_type == \"pre_ln\":\n",
    "    from tfdlg.models import PreLNDecoder\n",
    "    model = PreLNDecoder(config)\n",
    "elif model_type == \"post_ln\":\n",
    "    from tfdlg.models import PostLNDecoder \n",
    "    model = PostLNDecoder(config)\n",
    "elif model_type == \"transformers\":\n",
    "    model = TransformersGPT2(config)\n",
    "else:\n",
    "    raise Exception(\"Model type is wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "qualified-collectible",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T04:05:06.052288Z",
     "iopub.status.busy": "2021-03-07T04:05:06.051906Z",
     "iopub.status.idle": "2021-03-07T04:05:07.583183Z",
     "shell.execute_reply": "2021-03-07T04:05:07.582815Z"
    },
    "papermill": {
     "duration": 1.548191,
     "end_time": "2021-03-07T04:05:07.583261",
     "exception": false,
     "start_time": "2021-03-07T04:05:06.035070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pre_ln_decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder (Decoder)            multiple                  123614976 \n",
      "=================================================================\n",
      "Total params: 123,614,976\n",
      "Trainable params: 123,614,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None, config.context_size))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "authentic-ready",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-07T04:05:07.617350Z",
     "iopub.status.busy": "2021-03-07T04:05:07.616963Z",
     "iopub.status.idle": "2021-03-08T18:11:32.841130Z",
     "shell.execute_reply": "2021-03-08T18:11:32.840752Z"
    },
    "papermill": {
     "duration": 137185.242678,
     "end_time": "2021-03-08T18:11:32.841213",
     "exception": false,
     "start_time": "2021-03-07T04:05:07.598535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Epoch 1/10\n",
      "28504/28504 - 13715s - loss: 4.7492 - val_loss: 3.8091\n",
      "Epoch 2/10\n",
      "28504/28504 - 13705s - loss: 3.7234 - val_loss: 3.4365\n",
      "Epoch 3/10\n",
      "28504/28504 - 13709s - loss: 3.4467 - val_loss: 3.2843\n",
      "Epoch 4/10\n",
      "28504/28504 - 13701s - loss: 3.2927 - val_loss: 3.1959\n",
      "Epoch 5/10\n",
      "28504/28504 - 13659s - loss: 3.1860 - val_loss: 3.1417\n",
      "Epoch 6/10\n",
      "28504/28504 - 13728s - loss: 3.1045 - val_loss: 3.1050\n",
      "Epoch 7/10\n",
      "28504/28504 - 13760s - loss: 3.0388 - val_loss: 3.0735\n",
      "Epoch 8/10\n",
      "28504/28504 - 13790s - loss: 2.9839 - val_loss: 3.0507\n",
      "Epoch 9/10\n",
      "28504/28504 - 13749s - loss: 2.9378 - val_loss: 3.0340\n",
      "Epoch 10/10\n",
      "28504/28504 - 13662s - loss: 2.9006 - val_loss: 3.0232\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    epochs,\n",
    "    warmup_steps,\n",
    "    num_train_steps,\n",
    "    max_learning_rate,\n",
    "    clipnorm,\n",
    "    tensorboard_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "complete-scout",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T18:11:32.882765Z",
     "iopub.status.busy": "2021-03-08T18:11:32.882399Z",
     "iopub.status.idle": "2021-03-08T18:11:46.241784Z",
     "shell.execute_reply": "2021-03-08T18:11:46.241463Z"
    },
    "papermill": {
     "duration": 13.380668,
     "end_time": "2021-03-08T18:11:46.241869",
     "exception": false,
     "start_time": "2021-03-08T18:11:32.861201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0231893, 'perplexity': 20.55675, 'num_batches': 59, 'num_tokens': 241664}\n",
      "Validation PPL: 20.55675\n"
     ]
    }
   ],
   "source": [
    "from tfdlg.eval import perplexity\n",
    "\n",
    "print(\"Validation PPL:\", perplexity(model, valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interim-covering",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T18:11:46.284637Z",
     "iopub.status.busy": "2021-03-08T18:11:46.284244Z",
     "iopub.status.idle": "2021-03-08T18:11:46.646335Z",
     "shell.execute_reply": "2021-03-08T18:11:46.645948Z"
    },
    "papermill": {
     "duration": 0.384279,
     "end_time": "2021-03-08T18:11:46.646417",
     "exception": false,
     "start_time": "2021-03-08T18:11:46.262138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tfdlg.utils import save_model\n",
    "\n",
    "save_model(save_model_dir, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-alcohol",
   "metadata": {
    "papermill": {
     "duration": 0.019329,
     "end_time": "2021-03-08T18:11:46.685244",
     "exception": false,
     "start_time": "2021-03-08T18:11:46.665915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "papermill": {
   "duration": 141658.518571,
   "end_time": "2021-03-08T18:11:48.334475",
   "environment_variables": {},
   "exception": null,
   "input_path": "tfdlg_train.ipynb",
   "output_path": "output/tfmodel_train-pre_ln.ipynb",
   "parameters": {
    "batch_size": 4,
    "fp16": true,
    "model_type": "pre_ln"
   },
   "start_time": "2021-03-07T02:50:49.815904",
   "version": "2.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
